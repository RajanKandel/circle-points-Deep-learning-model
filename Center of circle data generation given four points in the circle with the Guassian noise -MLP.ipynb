{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the artificial dataset for points in the circle with the guassian noise\n",
    "<uo>\n",
    "    <li>Each training example consists of four features</li>\n",
    "    <li>Dataset consists of 1000 training example and 400 testing example</li>\n",
    "<uo>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random points in the circle (with uniform distribution) given the x, y cordinates \n",
    "# of the center of circle and radius\n",
    "import random \n",
    "import math\n",
    "import numpy as np\n",
    "from numpy.random import normal\n",
    "class pointInCircle:\n",
    "    def __init__(self):\n",
    "        self.r =random.uniform(0,101)\n",
    "        self.x =random.uniform(-50,50)\n",
    "        self.y =random.uniform(-50,50)\n",
    "            \n",
    "    def randPoint(self):  \n",
    "        '''generates center point and radius for the circle with uniform distribution in some range'''\n",
    "        self.r =random.uniform(0,101)\n",
    "        self.x =random.uniform(-50,50)\n",
    "        self.y =random.uniform(-50,50)\n",
    "        \n",
    "        #genearate three point in the circle with uniform distribution\n",
    "        points1 = []\n",
    "        for i in range(0,3):\n",
    "            #random theta\n",
    "            theta = random.uniform(0, 2*math.pi)\n",
    "            \n",
    "            # point in the circle given center (x,y)\n",
    "            #math logic: ((x+ rcos(theta)-x)^2 +(y+ rcos(theta)-y)^2 = r^2)\n",
    "            point = (self.x + self.r*math.cos(theta), self.y + self.r*math.sin(theta))\n",
    "            points1 += point                 \n",
    "        return(points1)\n",
    "    \n",
    "    def guassianDistPoint(self):\n",
    "        points2 = normal(loc=0, scale=0.01, size=6) #this is numpy array of 1X6\n",
    "        return(points2.tolist()) #converting numpy array to list\n",
    "        \n",
    "    \n",
    "    def pointsForTraining(self):\n",
    "        ''' three points in the circle as list in order x1, y1, x2, y2, x3, y3, x_center, y_center\n",
    "        with added guassian noise to points in circle only'''\n",
    "        sum_list = [a + b for a, b in zip(self.randPoint(),self.guassianDistPoint())]\n",
    "        sum_list.append(self.x)\n",
    "        sum_list.append(self.y)\n",
    "        #print(self.r)\n",
    "        return(sum_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = pointInCircle()\n",
    "#h.randPoint() \n",
    "#h.guassianDistPoint()\n",
    "#h.pointsForTraining()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data_list =[]\n",
    "for i in range (0,1000):\n",
    "    train_data_list.append(h.pointsForTraining())\n",
    "    \n",
    "test_data_list =[]\n",
    "for j in range(0,500):\n",
    "    test_data_list.append(h.pointsForTraining())\n",
    "    \n",
    "circles_train_df =pd.DataFrame(train_data_list, columns =['x1','y1','x2','y2','x3','y3','h','k'])\n",
    "circles_test_df =pd.DataFrame(test_data_list, columns =['x1','y1','x2','y2','x3','y3','h','k'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#circles_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#circles_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the pandas dataframe file to csv file\n",
    "circles_train_df.to_csv('circle_trainpoints.csv', index=False)\n",
    "circles_test_df.to_csv('circle_testpoints.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the data\n",
    "-seperating the features and labels in the training and testing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self,trainpoints, testpoints):\n",
    "        circle_train_df = pd.read_csv(trainpoints)\n",
    "        circle_test_df = pd.read_csv(testpoints)\n",
    "        \n",
    "        self.circle_train_df = circle_train_df.iloc[:,:].values\n",
    "        self.circle_test_df = circle_test_df.iloc[:,:].values\n",
    "        \n",
    "        self.x_circle_train_df = circle_train_df.iloc[:,0:6].values\n",
    "        self.y_circle_train_df = circle_train_df.iloc[:,6:7].values\n",
    "        \n",
    "        self.x_circle_test_df = circle_test_df.iloc[:,0:6].values        \n",
    "        self.y_circle_test_df = circle_test_df.iloc[:,6:7].values\n",
    "        \n",
    "        self.x_tensor_circle_train_df = torch.tensor(self.x_circle_train_df,dtype=torch.float64) \n",
    "        self.y_tensor_circle_train_df = torch.tensor(self.y_circle_train_df,dtype=torch.float64) \n",
    "        \n",
    "        self.x_tensor_circle_test_df = torch.tensor(self.x_circle_test_df,dtype=torch.float64) \n",
    "        self.y_tensor_circle_test_df = torch.tensor(self.y_circle_test_df,dtype=torch.float64) \n",
    "                \n",
    "           \n",
    "    def __len__(self):\n",
    "        return len(self.x_tensor_circle_train_df),len(self.y_tensor_circle_train_df), len(self.x_tensor_circle_test_df), len(self.y_tensor_circle_test_df)\n",
    "  \n",
    "    def __getitem__(self,idx):\n",
    "        return self.x_tensor_circle_train_df[idx],self.y_tensor_circle_train_df[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=MyDataset('circle_trainpoints.csv','circle_testpoints.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p.circle_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p.__getitem__(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    # The examples are read at random, in no particular order\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = torch.tensor(\n",
    "            indices[i: min(i + batch_size, num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read and print the first small batch of data examples. The shape of the features in each minibatch tells us both the minibatch size and the number of input features. Likewise, our minibatch of labels will have a shape given by batch_size.\n",
    "\n",
    "As we run the iteration, we obtain distinct minibatches successively until the entire dataset has been exhausted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.66320204e+01 -1.31991733e+01 -2.76935242e+01 -1.39482103e+01\n",
      "  -4.04483163e+01 -4.92888819e+01]\n",
      " [-4.19819915e+01  5.61925752e+00  5.99621019e+01 -2.36554231e+01\n",
      "   6.70389760e+01  1.84318922e+01]\n",
      " [-8.78005597e+01 -4.43963244e+01 -9.22416901e+01  8.29880817e+01\n",
      "   5.72432120e+01 -1.45319881e+01]\n",
      " [-6.81805651e-01 -7.29242259e+01 -3.94005196e+01  2.60980653e+00\n",
      "  -8.60439183e+01 -7.48370784e+01]\n",
      " [ 2.42479489e+01  1.21621218e+02  1.98952331e+01 -2.40281540e+01\n",
      "  -5.04331580e+01  6.77635538e+01]\n",
      " [ 5.21136063e+01 -2.34756065e+01  7.21089169e+01  9.98132866e-02\n",
      "   2.81837766e+01 -3.57382264e+01]\n",
      " [-3.30643081e+01 -3.03018071e+01 -2.25882484e+01  5.05196403e+00\n",
      "  -2.34618914e+01  5.17536117e+00]\n",
      " [ 1.26786938e+01 -7.89841854e+00 -3.43590848e+01  7.76143206e+00\n",
      "  -2.01006872e+01 -1.16129460e+01]\n",
      " [ 9.43365163e+01  3.24274676e+01  2.06208738e+01 -8.10011106e+01\n",
      "   7.20746576e+01  5.87424995e+01]\n",
      " [-2.49601710e+01  3.73365946e+01 -8.50363170e+01  2.81741926e+01\n",
      "   1.95555120e+01 -7.49410719e+00]] \n",
      " [[ -7.58574005]\n",
      " [ 13.25012543]\n",
      " [-25.7728839 ]\n",
      " [-43.94967302]\n",
      " [ 19.95400735]\n",
      " [  7.16173504]\n",
      " [-25.69438393]\n",
      " [ -6.35051338]\n",
      " [ 25.32113625]\n",
      " [-45.74449354]]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "features = p.x_circle_train_df\n",
    "#features=  torch.from_numpy(features)\n",
    "labels = p.y_circle_train_df \n",
    "#labels = torch.from_numpy(labels)\n",
    "\n",
    "for X, y in data_iter(batch_size, features, labels):\n",
    "    print(X, '\\n', y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the model parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initializing weights by sampling random numbers from a normal distribution with mean 0 and a standard deviation of 0.01, and setting the bias to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.normal(0, 0.01, size=(6,1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "updating them until they fit our data sufficiently well. Each update requires taking the gradient of our loss function with respect to the parameters. Given this gradient, we can update each parameter in the direction that may reduce the loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg(X, w, b):  #@save\n",
    "    return torch.matmul(X, w) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_hat, y):  \n",
    "    \"\"\"Squared loss.\"\"\"\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):  \n",
    "    '''Minibatch stochastic gradient descent.'''\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "#automatic differentiation, to compute the gradient.\n",
    "lr = 0.03\n",
    "num_epochs = 3\n",
    "net = linreg\n",
    "loss = squared_loss\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        l = loss(net(X, w, b), y)  # Minibatch loss in `X` and `y`\n",
    "        # gradient on `l` with respect to [`w`, `b`]\n",
    "        l.sum().backward()\n",
    "        sgd([w, b], lr, batch_size)  # Update parameters using their gradient\n",
    "    with torch.no_grad():\n",
    "        train_l = loss(net(features, w, b), labels)\n",
    "        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
